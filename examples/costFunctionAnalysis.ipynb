{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A quick demo for the analysis of KL divergence and Mahalanobis distance cost functions\n",
    "\n",
    "This script compares the differences in properties between our newly proposed KL divergence cost function (Eq. (S11)) and the Mahalanobis distance cost function (Eq. (S12)).\n",
    "\n",
    "In fact, they share the same fundamental objective: ensuring that the predictive function $f(\\cdot ; \\boldsymbol{\\theta})$ is close to a uniform vector $\\mathcal{U}$ over the context input space.\n",
    "\n",
    "Although the experiments results in Section 4.2~4.5 have verified that these two cost functions can improve model's performance, we also found that, in some experimental settings, such as the TinyImageNet detection task in Table 3,  KL divergence consistently outperforms the another one.\n",
    "\n",
    "We analyze that this is because the Mahalanobis distance may excessively increase or decrease the regularization strength of certain context samples, thereby further affecting the optimization process.\n",
    "\n",
    "Next, we will analyze this based on Appendix Fig. S5:\n",
    "\n",
    "![Alts](../figs/FigS5.png)\n",
    "\n",
    "Here, consider a binary classification task, where a training data includes positive (red) and negative (blue) classes are shown. The context set $\\mathcal{C}=\\left\\{\\widehat{\\mathbf{x}}_1, \\widehat{\\mathbf{x}}_2, \\widehat{\\mathbf{x}}_3, \\widehat{\\mathbf{x}}_4\\right\\}$ (green) consists of four points, with input feature $\\hat{\\mathbf{x}}_m \\in \\mathbb{R}^2$ in a 2D plane. The classifier is denoted as $\\boldsymbol{\\theta}$. For simplicity, its predictive function $f\\left(\\hat{\\mathbf{x}}_m ; \\boldsymbol{\\theta}\\right)$ is represented as $f\\left(\\hat{\\mathbf{x}}_m\\right)$. In binary classification, $[1,0]$ and $[0,1]$ indicate a negative and positive predictions, respectively, and $\\mathcal{U}=[0.5,0.5]$ is a uniform output. Ideally, we expect all context samples to be predicted as the uniform vector, which corresponds to a zero cost. Now, from left to right, the outputs of different samples change, resulting in the cost increasement.\n",
    "\n",
    "For (a) Mahalanobis distance cost function (Eq. (S12)):\n",
    "\n",
    "$$\n",
    "\\operatorname{cost}(\\mathcal{C}, \\boldsymbol{\\theta})=\\beta \\sum_{k=1}^K\\left(f(\\widehat{\\mathbf{X}} ; \\boldsymbol{\\theta})_k-\\mathcal{U}\\right)^{\\top} C^{-1}(\\widehat{\\mathbf{X}})\\left(f(\\widehat{\\mathbf{X}} ; \\boldsymbol{\\theta})_k-\\mathcal{U}\\right), \\text { where } C(\\widehat{\\mathbf{X}})=h(\\widehat{\\mathbf{X}}) h^{\\top}(\\widehat{\\mathbf{X}})+s \\mathbf{I}\n",
    "$$\n",
    "\n",
    "- The changes caused by $\\widehat{\\mathbf{x}}_2$ and $\\widehat{\\mathbf{x}}_4$ are significant (5.79 and 8.95), corresponding to a great regularization strength. Since they are relatively close to the training distribution, encouraging these inputs to output uniform vectors may affect prediction accuracy.\n",
    "- On the other hand, the changes in $\\widehat{\\mathbf{x}}_1$ and $\\widehat{\\mathbf{x}}_3$ are small (2.69). As they are relatively far from the training distribution, their weak influence may limit the model's ability to generalize OOD discrimination capabity to the whole OOD space.\n",
    "\n",
    "In contrast, for (b) our proposed KL divergence cost function (Eq. (S11)):\n",
    "\n",
    "$$\n",
    "\\operatorname{cost}(\\mathcal{C}, \\boldsymbol{\\theta})=\\beta \\sum_{m=1}^M \\operatorname{KL}\\left(f\\left(\\hat{\\mathbf{x}}_m ; \\boldsymbol{\\theta}\\right) \\| \\mathcal{U}\\right),  \\text { where } \\operatorname{KL}\\left(f\\left(\\widehat{\\mathbf{x}}_m ; \\boldsymbol{\\theta}\\right) \\| \\mathcal{U}\\right)=\\sum_{k=1}^K f\\left(\\widehat{\\mathbf{x}}_m ; \\boldsymbol{\\theta}\\right)_k \\log \\frac{f\\left(\\widehat{\\mathbf{x}}_m ; \\boldsymbol{\\theta}\\right)_k}{\\mathcal{U}_k}\n",
    "$$\n",
    "\n",
    "- This influence remains consistent (4.23), thus providing a more stable optimization process."
   ],
   "id": "d47c3769df7029c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**The following code provides an example of calculating these two types of cost functions when different sample points change.**",
   "id": "72cc6addde603c6a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:11.975685Z",
     "start_time": "2025-08-01T15:29:11.971749Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that the Mahalanobis distance cost function here is different from that in the script `helpers.py`, but they are equivalent. Here, the Mahalanobis distance is calculated using Appendix Eq. (S12), while in `helpers.py`, it is approximated as a _Gaussian process (GP)_ to accelerate computation process.",
   "id": "20624b66ce290c42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:12.167506Z",
     "start_time": "2025-08-01T15:29:12.156004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mahalanobis_distance_cost_function(preds_f, preds_feature, beta=1):\n",
    "    \"\"\"\n",
    "    :param preds_f: Model's logits output (without activation) [bts, K]\n",
    "    :param preds_feature:  Feature extractor output [bts, ndim]\n",
    "    :param beta: Cost function weight scalar parameter\n",
    "    \"\"\"\n",
    "    K = preds_f.size(1)  # K, number of classes\n",
    "    y = torch.ones_like(preds_f) / 2  # [bts, K] context set labels (we want all positions of the predicted output to be close to 0)\n",
    "    cov = preds_feature @ preds_feature.T  # [bts, bts]\n",
    "    cov += torch.eye(cov.size(0)).to(cov.device) * 0.05  # Add a small noise term to ensure positive definiteness\n",
    "    coov_inv = torch.inverse(cov)  # [bts, bts], the inverse of the covariance matrix\n",
    "\n",
    "    # Calculate the Mahalanobis distance cost function using Appendix Eq. (S12)\n",
    "    cost = torch.tensor(0.0, device=preds_f.device)  # 初始化代价函数\n",
    "    for k in range(K):\n",
    "        f_k = preds_f[:, k]  # [bts], the k-th class's logits output\n",
    "        y_k = y[:, k]  # [bts], the k-th class's target output\n",
    "        cost += beta * (f_k - y_k).T @ coov_inv @ (f_k - y_k)  # Appendix Eq. (S12)\n",
    "\n",
    "    return cost"
   ],
   "id": "6195a8c191cfce8b",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Case (i):** Changing the predictive function to $f(\\cdot ; \\boldsymbol{\\theta}) = [1, 0]$ for the first context point results in:\n",
    "- (a) Mahalanobis distance cost of $\\operatorname{cost}=2.69$\n",
    "- (b) KL divergence cost of $\\operatorname{cost}=4.23$."
   ],
   "id": "d4a2cb58d24cc315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:12.453200Z",
     "start_time": "2025-08-01T15:29:12.441611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([\n",
    "    [2.0, 4.0],\n",
    "    [4.0, 4.0],\n",
    "    [4.0, 2.0],\n",
    "    [2.0, 2.0]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "Y = torch.tensor([\n",
    "    [1, 0],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "uniform_dist = torch.Tensor(Y.size(0), 2).fill_((1. / 2))\n",
    "print(f\"Changing the first context point to [1, 0] results in:\")\n",
    "print(f\"Mahalanobis distance cost = {mahalanobis_distance_cost_function(Y, X) :.2f}\")\n",
    "print(f\"KL divergence cost = {torch.nn.functional.kl_div(torch.log(Y+1e-30), uniform_dist) :.2f}\")\n"
   ],
   "id": "85d2677de9b83c2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the first context point to [1, 0] results in:\n",
      "Mahalanobis distance cost = 2.69\n",
      "KL divergence cost = 4.23\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Case (ii):** Changing the predictive function to $f(\\cdot ; \\boldsymbol{\\theta}) = [1, 0]$ for the second context point results in:\n",
    "- (a) Mahalanobis distance cost of $\\operatorname{cost}=5.79$\n",
    "- (b) KL divergence cost of $\\operatorname{cost}=4.23$."
   ],
   "id": "88c3f3c9cdb5eab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:12.754519Z",
     "start_time": "2025-08-01T15:29:12.727388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([\n",
    "    [2.0, 4.0],\n",
    "    [4.0, 4.0],\n",
    "    [4.0, 2.0],\n",
    "    [2.0, 2.0]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "Y = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [1, 0],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "uniform_dist = torch.Tensor(Y.size(0), 2).fill_((1. / 2))\n",
    "print(f\"Changing the second context point to [1, 0] results in:\")\n",
    "print(f\"Mahalanobis distance cost = {mahalanobis_distance_cost_function(Y, X) :.2f}\")\n",
    "print(f\"KL divergence cost = {torch.nn.functional.kl_div(torch.log(Y+1e-30), uniform_dist) :.2f}\")"
   ],
   "id": "506e5e75e86bf6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the second context point to [1, 0] results in:\n",
      "Mahalanobis distance cost = 5.79\n",
      "KL divergence cost = 4.23\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Case (iii):** Changing the predictive function to $f(\\cdot ; \\boldsymbol{\\theta}) = [1, 0]$ for the third context point results in:\n",
    "- (a) Mahalanobis distance cost of $\\operatorname{cost}=2.69$\n",
    "- (b) KL divergence cost of $\\operatorname{cost}=4.23$."
   ],
   "id": "e0bc966fb4c585bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:13.041190Z",
     "start_time": "2025-08-01T15:29:13.022970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([\n",
    "    [2.0, 4.0],\n",
    "    [4.0, 4.0],\n",
    "    [4.0, 2.0],\n",
    "    [2.0, 2.0]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "Y = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "    [1, 0],\n",
    "    [0.5, 0.5],\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "uniform_dist = torch.Tensor(Y.size(0), 2).fill_((1. / 2))\n",
    "print(f\"Changing the third context point to [1, 0] results in:\")\n",
    "print(f\"Mahalanobis distance cost = {mahalanobis_distance_cost_function(Y, X) :.2f}\")\n",
    "print(f\"KL divergence cost = {torch.nn.functional.kl_div(torch.log(Y+1e-30), uniform_dist) :.2f}\")"
   ],
   "id": "c1c83d18f7116983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the third context point to [1, 0] results in:\n",
      "Mahalanobis distance cost = 2.69\n",
      "KL divergence cost = 4.23\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Case (iv):** Changing the predictive function to $f(\\cdot ; \\boldsymbol{\\theta}) = [1, 0]$ for the fourth context point results in:\n",
    "- (a) Mahalanobis distance cost of $\\operatorname{cost}=8.95$\n",
    "- (b) KL divergence cost of $\\operatorname{cost}=4.23$."
   ],
   "id": "395daf14a9650388"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:29:13.265271Z",
     "start_time": "2025-08-01T15:29:13.249571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([\n",
    "    [2.0, 4.0],\n",
    "    [4.0, 4.0],\n",
    "    [4.0, 2.0],\n",
    "    [2.0, 2.0]\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "Y = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "    [1, 0],\n",
    "],\n",
    "    dtype=torch.float32)\n",
    "uniform_dist = torch.Tensor(Y.size(0), 2).fill_((1. / 2))\n",
    "print(f\"Changing the third context point to [1, 0] results in:\")\n",
    "print(f\"Mahalanobis distance cost = {mahalanobis_distance_cost_function(Y, X) :.2f}\")\n",
    "print(f\"KL divergence cost = {torch.nn.functional.kl_div(torch.log(Y+1e-30), uniform_dist) :.2f}\")"
   ],
   "id": "5c1d2a2ef5bb47b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the third context point to [1, 0] results in:\n",
      "Mahalanobis distance cost = 8.95\n",
      "KL divergence cost = 4.23\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Through the above experiments, we found that the Mahalanobis distance cost function actually assigns different regularization strengths to different sample pairs, thereby affecting the model's optimization process.\n",
    "\n",
    "In contrast, the stable regularization strength of our newly proposed KL divergence cost function can provide more effective FSVI learning.\n",
    "\n",
    "The script `fsviContextSpecification.ipybn` provides a more detailed experimental evidence of this point."
   ],
   "id": "be50d3bcae6a7188"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
